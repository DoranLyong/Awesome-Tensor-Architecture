{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DoranLyong/Awesome-Tensor-Architecture/blob/main/pytorch_reference/simple_reference/05_Customizing/01_Customizing_PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers and Activations (p.136)\n",
    "* create custom ```layers``` and ```activations``` using functional definition (```nn.functional```)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your Linear-layer \n",
    "\n",
    "def linear(input, weight, bias=None): \n",
    "    # Y = X * W.t() + b \n",
    "\n",
    "    if input.dim() == 2 and bias is not None: \n",
    "        # fused-matrix operation is marginally faster \n",
    "        # b + X*W.t()\n",
    "        ret = torch.addmm(bias, input, weight.t()) # (ref) https://pytorch.org/docs/stable/generated/torch.addmm.html\n",
    "\n",
    "    else: \n",
    "        # X * W.t()\n",
    "        output = input.matmul(weight.t()) # (ref) https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "\n",
    "        if bias is not None: \n",
    "            output += bias\n",
    "        \n",
    "        ret = output \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0764,  6.3686,  3.0655],\n",
      "        [ 1.3306,  0.1079,  2.6226]])\n",
      "tensor([[-3.7471,  5.7551,  2.1965],\n",
      "        [ 0.9115, -0.0833,  3.0799]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.randn(2, 3) \n",
    "x = torch.randn(2, 3)\n",
    "W = mat2 = torch.randn(3, 3)\n",
    "\n",
    "output1 = linear(x, W, b)\n",
    "output2 = linear(x, W)\n",
    "\n",
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스로 디자인하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias):\n",
    "        \"\"\" y = x * W.t() + b\n",
    "            - in_features: size of each input sample\n",
    "            - out_features: size of each output sample\n",
    "            - bias: If set to ``False``, the layer will not learn an additive bias. (Default:=True) \n",
    "        \n",
    "            Attributes:\n",
    "            - weight: the learnable weights of the module of shape\n",
    "            - bias:   the learnable bias of the module of shape\n",
    "\n",
    "            Examples: \n",
    "                >> m = nn.Linear(20, 30)\n",
    "                >> input = torch.randn(128, 20)\n",
    "                >> output = m(input)\n",
    "                >> print(output.size())\n",
    "                torch.Size([128, 30])\n",
    "        \"\"\"\n",
    "\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features \n",
    "        self.weight = nn.parameter.Parameter( data=torch.Tensor(out_features, in_features), \n",
    "                                              requires_grad=True,\n",
    "                                            )\n",
    "\n",
    "        if bias: \n",
    "            self.bias = nn.parameter.Parameter(torch.Tensor(out_features))\n",
    "        \n",
    "        else: \n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5)) # (ref) https://pytorch.org/docs/stable/nn.init.html#\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "        \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor: \n",
    "        return linear(input, self.weight, self.bias) # (ref) https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html#torch.nn.functional.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "m = Linear(20, 30, True)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layer Example (Complex Linear) (p.138)\n",
    "* complex number ; ```a + jb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_linear(in_r, in_i, w_r, w_i, b_i, b_r): \n",
    "    out_r = (in_r.matmul(w_r.t()) - in_i.matmul(w_i.t()) + b_r) # real part  \n",
    "    out_i = (in_r.matmul(w_i.t()) - in_i.matmul(w_r.t()) + b_i) # imaginary part \n",
    "\n",
    "    return out_r, out_i "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스로 디자인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_r = nn.parameter.Parameter(torch.randn(out_features, in_features))\n",
    "        self.weight_i = nn.parameter.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias_r = nn.parameter.Parameter(torch.randn(out_features))\n",
    "        self.bias_i = nn.parameter.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, in_r, in_i):\n",
    "        return complex_linear(in_r, in_i, self.weight_r, self.weight_i, self.bias_r, self.bias_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLinearSimple(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ComplexLinearSimple, self).__init__()\n",
    "        self.fc_r = nn.Linear(in_features, out_features)\n",
    "        self.fc_i = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self,in_r, in_i):\n",
    "        return (self.fc_r(in_r) - self.fc_i(in_i), self.fc_r(in_i)+self.fc_i(in_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지, custom-layer 디자인하는 방법 정리 \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Activation Example (p.141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_relu(input, thresh=0.0):\n",
    "    return torch.where(input>thresh, input, torch.zeros_like(input)) # (ref) https://pytorch.org/docs/stable/generated/torch.where.html?highlight=torch%20where#torch.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7550,  0.9892],\n",
      "        [-0.0580, -0.4163]])\n",
      "tensor([[0.7550, 0.9892],\n",
      "        [0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2)\n",
    "print(x)\n",
    "print(my_relu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스 형태로 디자인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(nn.Module): \n",
    "    def __init__(self, thresh=0.0): \n",
    "        super(MyReLU, self).__init__()\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def forward(self, input): \n",
    "        return my_relu(input, self.thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8367,  0.4339],\n",
      "        [-0.9981, -0.5278]])\n",
      "tensor([[0.0000, 0.4339],\n",
      "        [0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "relu = MyReLU(thresh=0.0)\n",
    "x = torch.randn(2,2)\n",
    "\n",
    "print(x)\n",
    "print(relu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use activation? \n",
    "When building an ```NN```, it is common to use the functional version of the activation function. <br/>\n",
    "But, a class version can also be used if available. \n",
    "* functional version \n",
    "* class version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional version ReLU usage \n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out): \n",
    "        super(SimpleNet, self).__init__() \n",
    "        self.fc1 = nn.Linear(D_in, H) # input -> hidden \n",
    "        self.fc2 = nn.Linear(H, D_out) # hidden -> output \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # functional version ReLU\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class version ReLU usage \n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.net = nn.Sequential( nn.Linear(D_in, H),\n",
    "                                  nn.ReLU(),  # class version ReLU\n",
    "                                  nn.Linear(H, D_out),\n",
    "                                )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Activation Example (Complex ReLU) (p.142)\n",
    "* to handle complex values from the ```ComplexLinear``` layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional version \n",
    "\n",
    "def complex_relu(in_r, in_i): # input := (real, imaginary)\n",
    "    return (F.relu(in_r), F.relu(in_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class version \n",
    "\n",
    "class ComplexReLU(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(ComplexReLU, self).__init__()\n",
    "\n",
    "    def forward(self, in_r, in_i):\n",
    "        return complex_relu(in_r, in_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've learned how to create your own ```layers``` and ```activations```!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model Architectures (p.143)\n",
    "* build your ```AlexNet```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(  nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "                                        nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "                                        nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                    )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
    "\n",
    "        self.classifier = nn.Sequential(    nn.Dropout(),\n",
    "                                            nn.Linear(256 * 6 * 6, 4096), \n",
    "                                            nn.ReLU(inplace=True), \n",
    "                                            nn.Dropout(),\n",
    "                                            nn.Linear(4096, 4096), \n",
    "                                            nn.ReLU(inplace=True),\n",
    "                                            nn.Linear(4096, num_classes)\n",
    "                                        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool()\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The options here are pretrained and progress \n",
    "* ```torchvision.models.alexnet(pretrained=true)``` 이 동작하는 방식 설명 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "model_urls = { 'alexnet': \n",
    "                'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
    "            }\n",
    "\n",
    "def alexnet(pretrained=False, progress=True, **kwargs):\n",
    "    model = AlexNet(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(  model_urls['alexnet'],\n",
    "                                                progress=progress,\n",
    "                                            )\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /home/milky/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# to test code above\n",
    "model = alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Functions (p.145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variable to get code below to run\n",
    "outputs = torch.rand((10,10), requires_grad=True)\n",
    "targets = torch.rand((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1392, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# built-in version \n",
    "loss_fcn = nn.MSELoss()\n",
    "loss = loss_fcn(outputs, targets)\n",
    "loss.backward() # perform backpropagation \n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional version \n",
    "def mse_loss(input, target): \n",
    "    return ((input - target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1392, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1392, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = mse_loss(outputs, targets)\n",
    "print(loss)\n",
    "\n",
    "print(F.mse_loss(outputs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class version \n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return F.mse_loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1392, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = MSELoss()\n",
    "loss = criterion(outputs, targets)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss for Complex Numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_mse_loss(input_r, input_i, target_r, target_i):\n",
    "    return (((input_r-target_r)**2).mean(), # real part \n",
    "            ((input_i-target_i)**2).mean(), # imaginary part \n",
    "            )\n",
    "\n",
    "\n",
    "class ComplexMSELoss(nn.Module):\n",
    "    def __init__(self, real_only=False):\n",
    "        super(ComplexMSELoss, self).__init__()\n",
    "        self.real_only = real_only\n",
    "\n",
    "    def forward(self, input_r, input_i, target_r, target_i):\n",
    "        if (self.real_only):\n",
    "            return F.mse_loss(input_r, target_r)\n",
    "        else:\n",
    "            return complex_mse_loss(input_r, input_i, \n",
    "                                    target_r, target_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.1392, grad_fn=<MeanBackward0>), tensor(0.1392, grad_fn=<MeanBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# to test code above\n",
    "criterion = ComplexMSELoss()\n",
    "loss = criterion(outputs, outputs, targets, targets)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom optimizer Algorithms (p.147)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f38f9a92a40eacf7671051530596ac31a08fa1747600811db2b78ca4cf9fd4a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
