{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DoranLyong/Awesome-Tensor-Architecture/blob/main/pytorch_reference/simple_reference/06_PyTorch_Acceleration_and_Optimization/05_Pruning.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning (p.183)\n",
    "```Pruning``` is a technique that ```reduces``` the number of ```model parameters``` with ```minimal effect``` on performance. \n",
    "\n",
    "This allows you to deploy models with:\n",
    "* less ```memory```, \n",
    "* lower ```power usage```, \n",
    "* and reduced ```harward resources```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning model example \n",
    "__Pruning__ can be applied to an ```nn.module```. \n",
    "\n",
    "Since an ```nn.module``` may consist of a ```single layer```, ```multiple layers```, or an ```entire model```, <br/>\n",
    "```pruning``` can be applied to a single layer, multiple layers, or the entire model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model \n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d( F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d( F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LeNet5 model has five submodules - ```conv1```, ```conv2```, ```fc1```, ```fc2```, and ```fc3```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "torch.Size([6, 3, 5, 5])\n",
      "bias\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5().to(device)\n",
    "\n",
    "\n",
    "# Let's look at the parameters of the conv1 layer.\n",
    "\n",
    "for name, param in model.conv1.named_parameters():\n",
    "    print(name)\n",
    "    print(param.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Local and global pruning (p.184)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ex1) Local pruning: \n",
    "when only ```pruning``` a ```specific piece``` of our model. <br/>\n",
    "With this technique we can apply ```local pruning``` to a ```single layer``` or ```module```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "prune.random_unstructured(  model.conv1,     # target layer \n",
    "                            name = \"weight\", # parameter name\n",
    "                            amount=0.25,     \n",
    "                        )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.random_unstructured(  model.conv1,     # target layer \n",
    "                            name = \"bias\",   # parameter name\n",
    "                            amount=0.25,     \n",
    "                        )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune modules and parameters differently (p.185)\n",
    "\n",
    "for example: \n",
    "* prune by module or layer type \n",
    "* apply ```pruning``` to ```conv``` layers ```differently``` than ```linear``` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5().to(device)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d): \n",
    "        prune.random_unstructured(module, name='weight', amount=0.3) # Prune all 2D conv layers by 30%\n",
    "\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.random_unstructured(module,  name='weight', amount=0.5) # Prune all linear layers by 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ex2) Global pruning \n",
    "when applying a pruning method to the entire model. \n",
    "\n",
    "for example: \n",
    "* prune ```25%``` of our ```model's parameters``` ```globally```, which would probably result in different pruning rates for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5().to(device)\n",
    "\n",
    "parameters_to_prune = ( (model.conv1, 'weight'),\n",
    "                        (model.conv2, 'weight'),\n",
    "                        (model.fc1, 'weight'),\n",
    "                        (model.fc2, 'weight'),\n",
    "                        (model.fc3, 'weight'),\n",
    "                        )\n",
    "\n",
    "prune.global_unstructured(  parameters_to_prune,\n",
    "                            pruning_method=prune.L1Unstructured,\n",
    "                            amount=0.25\n",
    "                        ) \n",
    "\n",
    "# Here, we prune 25% of all the parameters in the entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Custom pruning methods (p.187)\n",
    "create your own pruning method. \n",
    "* use ```BasePruningMethod``` class in ```torch.nn.utils.prune```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPruningMethod(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = 'unstructured'\n",
    "\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        mask.view(-1)[::2] = 0\n",
    "        return mask\n",
    "\n",
    "\n",
    "def my_unstructured(module, name):\n",
    "    MyPruningMethod.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=400, out_features=120, bias=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LeNet5().to(device)\n",
    "\n",
    "my_unstructured(model.fc1, name='bias')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f38f9a92a40eacf7671051530596ac31a08fa1747600811db2b78ca4cf9fd4a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
