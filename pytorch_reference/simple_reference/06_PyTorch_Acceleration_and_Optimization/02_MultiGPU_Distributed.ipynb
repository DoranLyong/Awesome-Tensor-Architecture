{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DoranLyong/Awesome-Tensor-Architecture/blob/main/pytorch_reference/simple_reference/06_PyTorch_Acceleration_and_Optimization/02_MultiGPU_Distributed.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch on Multiple GPUs (Single Machine)\n",
    "1. data parallel processing \n",
    "2. model parallel processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = vgg16(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms as T \n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Dataset class returns a dataset object that includes : \n",
    "        - data and \n",
    "        - information about the data \n",
    "\"\"\"\n",
    "\n",
    "train_T = T.Compose([ T.RandomCrop(32, padding=4),\n",
    "                      T.RandomHorizontalFlip(), \n",
    "                      T.ToTensor(), \n",
    "                      T.Normalize( mean= (0.4914, 0.4822, 0.4465),\n",
    "                                   std=(0.2023, 0.1994, 0.2010)),\n",
    "                    ])\n",
    "\n",
    "train_data = CIFAR10(   root= \"./train\", \n",
    "                        train=True, \n",
    "                        download=True, \n",
    "                        transform=train_T, # set the transform parameter when creating the dataset \n",
    "                    )\n",
    "\n",
    "trainloader = DataLoader( train_data,  # dataset object \n",
    "                          batch_size=16, \n",
    "                          shuffle=True, \n",
    "                          num_workers=3,\n",
    "                        )                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parallel Processing (p.160)\n",
    "* ```multi-threaded``` approach uinsg ```nn.DataParallel```\n",
    "* ```multi-process``` approach using DDP (preferred) ★★★\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The multithreaded approach using nn.DataParallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This machine has 2 GPUs available.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (ref) https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html\n",
    "if torch.cuda.device_count() > 1: \n",
    "    print(\"This machine has\", torch.cuda.device_count(), \"GPUs available.\" )\n",
    "\n",
    "    model =  nn.DataParallel(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This multhithreaded approach is the simplest way to run on multiple GPUs; however, <br/>\n",
    "the ```multiprocess``` approach usually performs better, even on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The multiprocess approach using DDP (preferred, 이걸 써라 ★★★) - ([ref](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html))\n",
    "* distributed data processing (DDP)\n",
    "* it can be used with ```multiple processes``` on a ```single machine``` or across ```multiple machines```. \n",
    "\n",
    "\n",
    "Four steps:\n",
    "1. Initialize a process group using ```torch.distributed```.\n",
    "2. Create a local model using ```torch.nn.to()```.\n",
    "3. Wrap the model with DDP using ```torch.nn.parallel```.\n",
    "4. Spawn processes using ```torch.multiprocessing```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributed training loop for\n",
    "def dist_training_loop( rank, world_size, dataloader, model, loss_fn, optimizer): \n",
    "\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size) # initialize the process group.\n",
    "    \n",
    "    model = model.to(rank)  # move the model to a GPU with the ID of rank. \n",
    "    ddp_model = DDP(model, device_ids=[rank]) # Wrap the model in DDP \n",
    "\n",
    "    optim = optimizer(ddp_model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "    n_epochs = 10 \n",
    "    for epochs in range(n_epochs): \n",
    "        for input, labels in dataloader: \n",
    "\n",
    "            input = input.to(rank)\n",
    "            labels = labels.to(rank) # move inputs and labels to the GPU with the ID of rank. \n",
    "\n",
    "            outputs = ddp_model(input) # call the DDP model for the forward pass \n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optim.zero_grad() \n",
    "            loss.backward()\n",
    "            optim.step() \n",
    "    \n",
    "    dist.destroy_process_group()  # cealnup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === run main === # \n",
    "world_size = 2 \n",
    "\n",
    "mp.spawn(   dist_training_loop,\n",
    "            args=(world_size,),\n",
    "            nprocs=world_size,\n",
    "            join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model does not fit into a single GPU or you are using smaller batch sizes, <br/> \n",
    "you may consider using model parallel processing instead of data parallel processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parallel Processing (p.164) - ([ref](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet, Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10  # CIFAR-10\n",
    "\n",
    "\n",
    "class ModelParallelResNet50(ResNet):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ModelParallelResNet50, self).__init__( Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)\n",
    "\n",
    "        self.seq1 = nn.Sequential( \n",
    "            self.conv1,\n",
    "            self.bn1,\n",
    "            self.relu,\n",
    "            self.maxpool,\n",
    "\n",
    "            self.layer1,\n",
    "            self.layer2\n",
    "        ).to('cuda:0')\n",
    "\n",
    "        self.seq2 = nn.Sequential(\n",
    "            self.layer3,\n",
    "            self.layer4,\n",
    "            self.avgpool,\n",
    "        ).to('cuda:1')\n",
    "\n",
    "        self.fc.to('cuda:1')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq2(self.seq1(x).to('cuda:1'))\n",
    "        return self.fc(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, Avg_loss: 2.26372942527771\n",
      " Epoch: 1, Avg_loss: 2.058728328475952\n",
      " Epoch: 2, Avg_loss: 1.9407443102645874\n"
     ]
    }
   ],
   "source": [
    "model = ModelParallelResNet50() \n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "n_epochs = 3\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    batch_loss = 0.0 \n",
    "\n",
    "    for input, labels in trainloader: \n",
    "        input = input.to(\"cuda:0\")\n",
    "        labels = labels.to(\"cuda:1\") \n",
    "\n",
    "        outputs = model(input) \n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # === accumulate batch loss \n",
    "        batch_loss += loss.item()         \n",
    "\n",
    "    print(f\" Epoch: {epoch}, Avg_loss: {batch_loss / len(trainloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Combined DDP and Model Parallel Processing (p.167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f38f9a92a40eacf7671051530596ac31a08fa1747600811db2b78ca4cf9fd4a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
