# Attention and the Transformer

Running by  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DoranLyong/Awesome-Tensor-Architecture/blob/main/pytorch_reference/NYU-DL/12-Attention-and-Transformer/01-Attention-and-Transformer.ipynb)



â€» ```(Soft)ArgMax ```   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DoranLyong/Awesome-Tensor-Architecture/blob/main/pytorch_reference/NYU-DL/12-Attention-and-Transformer/(Soft)Argmax.ipynb)





***

### Reference 

1. [10 - Self / Cross,  Hard / Soft attention and the Transformer, NYU-DLSP21](https://youtu.be/fEVyfT-gLqQ)
2. [NYU-DLSP course page](https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/) 
3. [Attention Is All You Need, Yannic Kilcher](https://youtu.be/iDulhoQ2pro)

