{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "\n",
    "from SmithZero import D2torchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4355a6d390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10 \n",
    "import torchvision.transforms as T \n",
    "\n",
    "\n",
    "# === data transformation === # \n",
    "normalize = T.Normalize(mean= (0.4914, 0.4822, 0.4465),\n",
    "                        std=(0.2023, 0.1994, 0.2010))\n",
    "\n",
    "train_T = T.Compose([   T.RandomCrop(32, padding=4), \n",
    "                        T.RandomHorizontalFlip(), \n",
    "                        T.ToTensor(),\n",
    "                        normalize,\n",
    "                    ])               \n",
    "\n",
    "test_T = T.Compose([    T.Resize(32),\n",
    "                        T.ToTensor(),\n",
    "                        normalize,\n",
    "                    ])\n",
    "\n",
    "\n",
    "# === dataset object === # \n",
    "train_dataset = CIFAR10 (  root=\"./dataset/train\",\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=train_T )\n",
    "\n",
    "test_dataset = CIFAR10 (   root=\"./dataset/test\",\n",
    "                            train=False,\n",
    "                            download=True, \n",
    "                            transform=test_T )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(train_dataset, \n",
    "                        batch_size=64,\n",
    "                        shuffle=True, \n",
    "                        num_workers=4\n",
    "                            )\n",
    "\n",
    "testloader = DataLoader(test_dataset, \n",
    "                        batch_size=64,\n",
    "                        shuffle=False, \n",
    "                        num_workers=4\n",
    "                            )                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Auxiliary Classifiers (Side-Heads) for ```Inception``` model\n",
    "\n",
    "* <b>auxiliary classifier</b> = <b>side-heads</b>\n",
    "* <b>main classifier</b>\n",
    "\n",
    "![Inception model](./page_img/inception_model.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The <b>cross-entropy loss</b> was also computed independently for <b>each one of the three classifiers</b> and <b>added together</b> to the total loss (although auxiliary losses were multiplied by a factor of 0.3).\n",
    "* The auxiliary classifiers (and losses) were used during training time only.\n",
    "* During the <b>evaluation</b> phase, <b>only the logits produced by the main classifier were considered</b>.\n",
    "\n",
    "<br/>\n",
    "\n",
    "This technique was originally developed to mitigate the ```vanishing gradients``` problem (more on that in the next chapter), <br/>\n",
    "but it was later found that the ```auxiliary classifiers``` are more likely to have a ```regularizer effect``` instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception v3 \n",
    "The third version of the Inception model (```inception_v3```), available as a pre-trained model in PyTorch, has only <b>one auxiliary classifier</b> instead of two, <br/>\n",
    "but we still need to <b>make some adjustments</b> if weâ€™re using this model for transfer learning. \n",
    "\n",
    "<br/>\n",
    "\n",
    "```inception_v3``` setup for transfer learning: \n",
    "* <b>load</b> the pre-trained model\n",
    "* <b>freeze</b> its layer \n",
    "* <b>replace the layers</b> for both ```main``` and ```auxiliary``` classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3\n",
    "\n",
    "# load the pre-trained model \n",
    "model = inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze your model \n",
    "\n",
    "def freeze_model(model):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "freeze_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AuxLogits.conv0.conv.weight\n",
      "AuxLogits.conv0.bn.weight\n",
      "AuxLogits.conv0.bn.bias\n",
      "AuxLogits.conv1.conv.weight\n",
      "AuxLogits.conv1.bn.weight\n",
      "AuxLogits.conv1.bn.bias\n",
      "AuxLogits.fc.weight\n",
      "AuxLogits.fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters(): \n",
    "    if \"AuxLogits\" in name: \n",
    "        print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=1000, bias=True)\n",
      "Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.AuxLogits.fc)   # auxiliary classifier \n",
    "print(model.fc)             # main classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the layers for both main and auxiliary classifiers \n",
    "n_classes = 10\n",
    "\n",
    "model.AuxLogits.fc = nn.Linear(768, n_classes)\n",
    "model.fc = nn.Linear(2048, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 350, 350])\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.randn((1, 3, 350, 350))\n",
    "print(x_test.size())\n",
    "\n",
    "main, aux = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "print(main.size())\n",
    "print(aux.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Loss (```joint losses```)\n",
    "Unfortunately, we cannot use the standard ```cross-entropy loss``` because the Inception model ```outputs two tensors```, <br/>\n",
    "one for each classifier (although it is possible to force it to return only the main classifier by setting its ```aux_logits``` argument to ```False```). <br/>\n",
    "\n",
    "But we can create a simple ```function``` that can handle ```multiple outputs```, computing the ```corresponding losses``` and returning their total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception Loss with Side-heads \n",
    "\n",
    "def inception_loss(model_outputs, labels):\n",
    "    try: \n",
    "        main, aux = model_outputs\n",
    "\n",
    "    except ValueError: \n",
    "        main = model_outputs  # output for main-classifier \n",
    "        aux = None            # output for aux-classifier \n",
    "        loss_aux = 0 \n",
    "    \n",
    "    # *** get loss out of main-classifier\n",
    "    multi_loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    loss_main = multi_loss_fn(main, labels) # for main-classifier \n",
    "\n",
    "    # *** get loss out of aux-classifier\n",
    "    if aux is not None: \n",
    "        loss_aux = multi_loss_fn(aux, labels)\n",
    "    \n",
    "    return loss_main + 0.4 * loss_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f38f9a92a40eacf7671051530596ac31a08fa1747600811db2b78ca4cf9fd4a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
