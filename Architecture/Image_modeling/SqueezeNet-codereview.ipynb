{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SqueezeNet-코드분석.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNkmaR+N+G4URgHrvNGz/zy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Reference \n","* [paper in arXiv2016](https://arxiv.org/abs/1602.07360)\n","* Reference to [MLT Artificial Intelligence's YouTube](https://www.youtube.com/watch?v=DbPbYGopC2c)\n","* Referemce to [gsp-27's github](https://github.com/gsp-27/pytorch_Squeezenet/blob/master/model.py)\n","* Reference to [2D-SqueezeNet of pytorch vision](https://github.com/pytorch/vision/blob/main/torchvision/models/squeezenet.py)\n","* Reference to [VITALab's blog](https://vitalab.github.io/article/2018/03/15/squeezeNet.html)\n","* Reference to [arvention's github](https://github.com/arvention/SqueezeNet-PyTorch/blob/master/model.py)\n","* Reference to [gsp-27's github](https://github.com/gsp-27/pytorch_Squeezenet/blob/master/model.py)\n","* Reference to [JINSOL KIM's blog](https://gaussian37.github.io/dl-concept-squeezenet/)\n","* Reference to [PR-144 Youtube](https://youtu.be/WReWeADJ3Pw)\n","* Reference to [Matthijs Hollemans's post](https://machinethink.net/blog/mobile-architectures/)"],"metadata":{"id":"LDN-kkvYu9L3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VLvDWgMu6SE"},"outputs":[],"source":["import numpy as np \n","\n","import torch \n","import torch.nn as nn \n","import torch.nn.init as init"]},{"cell_type":"code","source":["# (ref) https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"metadata":{"id":"1Bbzxg7W9mtY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Contribution and Background\n","\n","<b>[의의]</b>\n","* ImageNet에서 ```AlexNet 수준의 accuracy```를 달성하였지만 ```AlexNet 보다 50배 작은 파라미터 수```를 이용하였다는 것에 의미가 있음. \n","* 또한, hyper-parameter를 어떻게 조합하느냐에 따라 ```성능과 파라미터의 개수 사이의 관계```를 확인한 것에 의의가 있음 \n","\n","<b>[배경]</b>\n","* 시간이 흐를수록 딥러닝 모델의 ```accuracy가 좋아지는 반면```, 모델의 ```사이즈가 점점 더 커지는 문제``` 발생. \n","* 이는 모바일 및 임베딩 환경 같이 저사양 컴퓨팅 환경에서 사용하기 어렵게 만듬. \n","* 이러한 배경 속에서 ```quantization```이나 ```모델 경량화```에 많은 관심이 생기게 되었습니다."],"metadata":{"id":"QKnhO6Me4zVy"}},{"cell_type":"markdown","source":["# 2. Preliminaries\n","스퀴즈넷에서는 <b>micro architecture</b>와 <b>macro architecture</b> 측면으로 나누어서 전체 내용을 설명하고 있음. <br/>\n","\n","<b>[micro architecture]</b>\n","* $1 \\times 1 $ convolution \n","* $3 \\times 3 $ convolution \n","* skip connection \n","\n","<b>[macro architecture]</b>\n","* block stacking"],"metadata":{"id":"_d4j2U0b7eTv"}},{"cell_type":"markdown","source":["## 2-1. micro architecture "],"metadata":{"id":"I7iJYqpT8CRw"}},{"cell_type":"markdown","source":["### (1) $1\\times1$ conv (=sperable/point-wise convolution)\n","* 1x1 convolution filter가 channel 방향의 multi-layer perceptron과 같은 역할을 한다는 점. \n","    * (왜?) 1x1 convolution의 경우 <b>각각의 채널에</b> element-wise로 multiplication 한 후에 합하게 된다. 이 연산은 채널 방향의 multi-layer perceptron 즉, FC layer와 같다는 뜻\n","* $1\\times1$ filter 하나가 채널 방향으로  ```dot product``` 함으로써 perceptron node 하나를 만드는 것과 같다.\n","    * 대신 파라미터 개수 = $1\\times1\\times\\text{num_channel}$로써 파라미터 개수가 dense layer 보다 작다. \n","\n","<img src=\"https://miro.medium.com/max/1400/1*dNaikOfrGzUaJ2EzRIl4tw.png\" width=480> <br/>\n","(source: [Raj Sakthi's medium](https://medium.com/analytics-vidhya/talented-mr-1x1-comprehensive-look-at-1x1-convolution-in-deep-learning-f6b355825578))\n"],"metadata":{"id":"wt1CTOc98fQz"}},{"cell_type":"code","source":["input = torch.randn(1, 192, 64, 64)\n","_, n_c, h, w = input.size() \n","\n","pw_conv = nn.Conv2d(in_channels= n_c,\n","                    out_channels = 1, \n","                    kernel_size=1, \n","                    stride=1,\n","                    padding=0,\n","                    bias=False,\n","                    )\n","\n","output = pw_conv(input)\n","print(output.size())\n","print(f\"# of learnable parameters: {count_parameters(pw_conv)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"luP8t2S_9glQ","executionInfo":{"status":"ok","timestamp":1649298142081,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍윤건우[ 대학원박사과정수료연구(재학) / 컴퓨터학과 ]","userId":"00259121442590193501"}},"outputId":"6e131c2d-8822-42f3-eed6-d72453b6c98f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 64, 64])\n","# of learnable parameters: 192\n"]}]},{"cell_type":"markdown","source":["### (2) $3\\times3$ conv\n","\n","<b>[[Filter factorization](https://medium.com/@dmangla3/designing-faster-neural-networks-e1f1dc026533)]</b><br/>\n","어떤 필터를 사용할 것인가? \n","* 초기 딥러닝 모델에서는 넓은 receptive field를 위해 $5\\times5$ 혹은 $7\\times7$ filter를 사용했다. 하지만, 요즘은 $3\\times3$ filter를 사용하는 것이 주류다. \n","* 어찌 보면 잘못된 선택 처럼 보이지만, 보다 작은 필터(e.g., $3\\times3$)를 레이어 방향으로 순차적으로 적용하면 $5\\times5$ filter 를 적용할 때와 똑같은 효과의 receptive fields를 얻을 수 있다. \n","\n","<img src=\"https://miro.medium.com/max/1314/1*PsvllwB__wjKpHdXTr0oKw.jpeg\" width=480> <br/>\n","(source:[Amit Singh Bhatti's medium](https://medium.datadriveninvestor.com/how-neural-nets-fell-in-love-with-computer-hardwares-79a4617c8f3a), Decomposing larger filters into smaller filters.)\n","\n","* 위 그림의 경우 $5\\times5$ filter를 사용하려면 25-weight 이 필요하다. \n","* 반면, $3\\times3$ filter 두 개로 분해하면 ($3\\times3 + 3\\times3$)=18-weight 로 파라미터 개수가 더 적다. \n","\n","<b>[Other benefits]</b><br/>\n","* 사이즈가 작은 만큼 더욱 지엽적으로(highly local) 특징을 뽑을 수 있다. 즉, 작고 세밀한(fine-grained) 특징을 잡기에 유리하다. \n","* Kernel을 연쇄적으로 사용하면(using kernel sequentially = increasing number of layers) 더 복잡한 특징(feature)를 학습할 수 있다.\n","    * $3\\times3$ filter를 사용하면 activation function을 2번 거친다.\n","    * $5\\times5$ filter를 사용하면 activation function을 1번 거친다. \n","\n","<b>[Example]</b>\n","* 1개 layer의 5x5 필터는 2개 layer의 3x3 필터로 대체될 수 있음.\n","\n","<img src=\"https://oi.readthedocs.io/en/latest/_images/conv_factorization.png\" width=480><br/>\n","(source:[bskyvision's blog](https://bskyvision.com/504))\n"],"metadata":{"id":"pTGAaPiyABlK"}},{"cell_type":"code","source":["input = torch.randn(1, 1, 7, 7)\n","_, n_c, h, w = input.size() \n","\n","\n","conv_5x5 = nn.Conv2d(in_channels= n_c,\n","                    out_channels = 1, \n","                    kernel_size=5, \n","                    stride=1,\n","                    padding=0,\n","                    bias=False,\n","                    )\n","output_5x5 = conv_5x5(input)\n","print(output_5x5.size())\n","print(f\"# of learnable parameters: {count_parameters(conv_5x5)}\")\n","\n","\n","conv_3x3 = nn.Sequential(\n","    nn.Conv2d(n_c, 1, 3, 1, 0, bias=False),\n","    nn.Conv2d(1, 1, 3, 1, 0, bias=False),\n",")\n","output_3x3 = conv_3x3(input)\n","print(output_3x3.size())\n","print(f\"# of learnable parameters: {count_parameters(conv_3x3)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ruqXOegKM64","executionInfo":{"status":"ok","timestamp":1649298142081,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍윤건우[ 대학원박사과정수료연구(재학) / 컴퓨터학과 ]","userId":"00259121442590193501"}},"outputId":"3a2f0b83-23a3-491a-b3e5-34d306b5f600"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 3, 3])\n","# of learnable parameters: 25\n","torch.Size([1, 1, 3, 3])\n","# of learnable parameters: 18\n"]}]},{"cell_type":"markdown","source":["### (3) skip connection \n","```skip connection```의 관점은 ResNet과 Segmentation에서 볼 수 있음. </br> \n","\n","<b>[Segmentation에서의 관점]</b>\n","* Segmentation에서는 resolution 정보가 굉장히 중요함. 하지만, deep layer로 계층이 내려갈수록 downsampling 과정에서 본래의 spatial resolution 정보가 소실됨.  \n","* 이를 다시 회복하기 위한 upsampling 과정에서 long skip connection을 통해 인코딩 전의 같은 레이어 레벨의 resolution 정보를 더해 줌으로써 완전한 spatial resolution을 회복하는 데 도움을 줌.\n","\n","<img src=\"https://theaisummer.com/static/2c373d3667071700748bf451c4e62b78/7f018/long-skip-connection.jpg\" width=480> </br>\n","(source: [AI SUMMER](https://theaisummer.com/skip-connections/))\n","\n","\n","</br>\n","\n","\n","<b>[ResNet에서의 관점]</b>\n","* 위와 비슷한 컨셉으로 작동한다. \n","* Vanishing gradient 문제를 skip connection을 통해 개선한 것에 의미가 있고, \n","* 혹자는 Optimization 관점에서 loss function을 smoothing하는 효과 때문에 global minima을 더욱 잘 찾을 수 있게 만든다고 주장함.\n","\n","<img src=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/images/pmlc_0331.png?raw=true\" width=480> <br/>\n","(source: [paper](https://arxiv.org/abs/1712.09913))\n","\n","<img src=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/images/pmlc_0327.png?raw=true\" width=300> <br/>\n","(A residual block: [practical-ml-vision-book](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/images/pmlc_0327.png))\n","\n"],"metadata":{"id":"-n5MpU10LWoE"}},{"cell_type":"code","source":["# (ref) https://coding-yoon.tistory.com/141\n","x = torch.randn(1,256,224,224)\n","_, n_c, h, w =  x.size()\n","\n","conv_path = nn.Sequential(\n","    nn.Conv2d(n_c, 64, kernel_size=1, stride=1, padding=0, bias=False),\n","    nn.ReLU(),\n","    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n","    nn.ReLU(),\n","    nn.Conv2d(64, n_c, kernel_size=1, stride=1, padding=0, bias=False),\n",")\n","relu = nn.ReLU()\n","\n","out = conv_path(x) # C(x)\n","out = out + x  # C(x) + x ; residual mapping \n","out = relu(out)\n","\n","print(out.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvuXhezkll7J","executionInfo":{"status":"ok","timestamp":1649298142419,"user_tz":-540,"elapsed":341,"user":{"displayName":"‍윤건우[ 대학원박사과정수료연구(재학) / 컴퓨터학과 ]","userId":"00259121442590193501"}},"outputId":"c8cbf2f7-b90a-413d-ff72-d34e7e9b79e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 256, 224, 224])\n"]}]},{"cell_type":"markdown","source":["## 2-2. macro architecture \n","```macro architecture```란 ```block``` 모듈을 쌓는 컨셉이다 (i.e., block stacking).\n","* Inception, MobileNet, ShuffleNet 등에서 모듈 단위의 블록(block)을 쌓아서 네트워크 전체를 구성한다. \n","* SqueezeNet도 이와 유사한 개념을 적용한다. \n","\n","<img src=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/images/pmlc_0324.png?raw=true\" width=480> </br>\n","(source: [practical-ml-vision-book](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/images/pmlc_0327.png))"],"metadata":{"id":"vXw7YvL_oAHH"}},{"cell_type":"markdown","source":["# 3. Architecture\n","* Reference to [Hao Gao's medium](https://medium.com/@smallfishbigsea/notes-of-squeezenet-4137d51feef4) \n","\n","SqueezeNet을 구성하는 전략은 총 세가지다: \n","\n","1. $3\\times3$ filter를 $1\\times1$ filter로 대체한다; \n","    * [#channel, H, W] → [#channel', H, W] 형태의 활성화 맵을 만들거면 $1 \\times 1$ filter를 사용하는 것이 파라미터의 개수도 적기 때문에 저렴함 (전략 3과 관련). \n","2. $3\\times3$ filter의 채널 수를 줄인다.\n","    * $3\\times3$ filter의 파라미터 개수 = $3\\times3\\times \\text{num_channels}$ 이므로 채널이 늘어나면 파라미터 개수도 금증함. \n","3. Convolution layer의 활성화 맵(activation map)을 크게 만들기 위해 downsample을 늦게 진행한다. \n","    * 네트워크 앞쪽에서 downsample을 하게 되면 activation map의 크기가 줄어 연산량이 줄기 때문에 처리 속도는 빨라짐 \n","    * 하지만, 그만큼 activation map에서 정보가 소실되기 때문에 accuracy가 떨어짐 \n","    * 따라서, downsample을 늦게 하여 accuracy를 높이는 전략을 취함 \n","\n","</br>\n","\n","```전략 1,2```는 필터를 효율적으로 사용하여 최대한 <b>파라미터 개수를 줄이는 것을 목적</b>으로 한다. 이 과정에서 ```전략 3```은 <b>최대한 성능을 내기 위해 activation map의 크기를 유지하는 것</b>으로 전체 전략을 정리할 수 있다.  "],"metadata":{"id":"BD3P-QyA6lij"}},{"cell_type":"markdown","source":["## 3-1. Fire Module\n","* Reference to [Machine-Learning-Tokyo's github](https://github.com/Machine-Learning-Tokyo/DL-workshop-series/blob/master/Part%20I%20-%20Convolution%20Operations/ConvNets.ipynb)\n","\n","SqueezeNet에서 block stacking을 위해 사용하는 모듈이다. <br>\n","\n","\n","* $1 \\times 1$ conv filter를 통해 채널을 압축한다. <b>[Squeeze layer]</b>\n","\n","\n","* $1 \\times 1$ 및 $3 \\times 3$ conv filter를 통해 다시 채널을 팽창시킨다 <b>[Expand layer]</b>\n","* Inception 모듈과 같이 $1 \\times 1$ 및 $3 \\times 3$ conv filter 로 구성된 <b>Multiple path</b>를 줌으로써 다양한 특징을 학습할 수 있게 했다. \n","* Activation function은 ReLU를 사용한다. \n","\n","\n","\n","<img src=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/images/pmlc_0325.png?raw=true\" width=\"480\" /> </br>\n","(source: [practical-ml-vision-book](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/images/pmlc_0327.png))"],"metadata":{"id":"HfYHQefV3dpO"}},{"cell_type":"code","source":["# (ref) https://github.com/pytorch/vision/blob/main/torchvision/models/squeezenet.py\n","\n","class Fire(nn.Module):\n","    def __init__(self, inplanes: int, squeeze_planes: int, \n","                 expand1x1_planes: int, expand3x3_planes: int) -> None:\n","        super(Fire, self).__init__()    \n","\n","        # squeeze layer (채널 축소)\n","        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n","        self.squeeze_activation = nn.ReLU(inplace=True)\n","        # expand layer (채널 팽창)\n","        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1)\n","        self.expand1x1_activation = nn.ReLU(inplace=True)\n","        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1)\n","        self.expand3x3_activation = nn.ReLU(inplace=True)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.squeeze_activation(self.squeeze(x))\n","        return torch.cat(\n","            [self.expand1x1_activation(self.expand1x1(x)), \n","             self.expand3x3_activation(self.expand3x3(x))], \n","             axis=1)"],"metadata":{"id":"dFrEJWxexrXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input = torch.randn(1, 96, 224, 224)\n","\n","fire = Fire(96, 16, 64, 64) # 96 -> 16 축소 \n","                            # 16 -> 64 팽창 \n","                            # concate; 64+64=128\n","out = fire(input)\n","print(out.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VgPhCsHky2-_","executionInfo":{"status":"ok","timestamp":1649298142420,"user_tz":-540,"elapsed":6,"user":{"displayName":"‍윤건우[ 대학원박사과정수료연구(재학) / 컴퓨터학과 ]","userId":"00259121442590193501"}},"outputId":"41cbede7-b7e6-4134-e144-287c33130677"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 128, 224, 224])\n"]}]},{"cell_type":"markdown","source":["## 3-2. Block stacking\n","<img src=\"https://i.stack.imgur.com/0pOi4.png\" width=640> </br>\n","(source: [StackExchange](https://datascience.stackexchange.com/questions/33114/what-does-depth-mean-in-the-squeezenet-architectural-dimensions-table))"],"metadata":{"id":"jqIEKN4d5Lbp"}},{"cell_type":"code","source":["class SqueezeNet(nn.Module):\n","    def __init__(self, version: str = \"1_0\", num_classes: int = 1000, dropout: float = 0.5) -> None:\n","        super(SqueezeNet, self).__init__()\n","        \n","        self.num_classes = num_classes\n","\n","        if version == \"1_0\":\n","            self.features = nn.Sequential(\n","                # 첫 input을 처리할 때에만 예외적인 Convolution 연산과 MaxPool을 적용한 뒤\n","                # 이후에는 Fire Module을 계속 적용함.  \n","                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n","                \n","                # (입력채널, squeeze 채널, 1x1 expand 채널, 3x3 expand 채널)\n","                Fire(96, 16, 64, 64),\n","                Fire(128, 16, 64, 64),\n","                Fire(128, 32, 128, 128),\n","                \n","                # late downsampling(MaxPool)을 적용함\n","                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n","                Fire(256, 32, 128, 128),\n","                Fire(256, 48, 192, 192),\n","                Fire(384, 48, 192, 192),\n","                Fire(384, 64, 256, 256),\n","                # late downsampling(MaxPool)을 적용함\n","                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n","                Fire(512, 64, 256, 256),\n","            )\n","        elif version == \"1_1\":\n","            self.features = nn.Sequential(\n","                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n","                Fire(64, 16, 64, 64),\n","                Fire(128, 16, 64, 64),\n","                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n","                Fire(128, 32, 128, 128),\n","                Fire(256, 32, 128, 128),\n","                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n","                Fire(256, 48, 192, 192),\n","                Fire(384, 48, 192, 192),\n","                Fire(384, 64, 256, 256),\n","                Fire(512, 64, 256, 256),\n","            )\n","        else:\n","            # FIXME: Is this needed? SqueezeNet should only be called from the\n","            # FIXME: squeezenet1_x() functions\n","            # FIXME: This checking is not done for the other models\n","            raise ValueError(f\"Unsupported SqueezeNet version {version}: 1_0 or 1_1 expected\")\n","\n","        # Final convolution is initialized differently from the rest\n","        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=dropout), final_conv, nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                if m is final_conv:\n","                    init.normal_(m.weight, mean=0.0, std=0.01)\n","                else:\n","                    init.kaiming_uniform_(m.weight)\n","                if m.bias is not None:\n","                    init.constant_(m.bias, 0)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return torch.flatten(x, 1)"],"metadata":{"id":"pXncmP5i3XBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input = torch.randn(1, 3, 224, 224)\n","model = SqueezeNet(num_classes= 1000, dropout = 0.5)\n","\n","out = model(input)\n","\n","print(out.size())\n","print(f\"# of learnable parameters: {count_parameters(model)/1e6}M\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfkGdKvm3NSB","executionInfo":{"status":"ok","timestamp":1649298142754,"user_tz":-540,"elapsed":338,"user":{"displayName":"‍윤건우[ 대학원박사과정수료연구(재학) / 컴퓨터학과 ]","userId":"00259121442590193501"}},"outputId":"9e44b6c6-6b4b-4966-9ebc-2cac0e79cefe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1000])\n","# of learnable parameters: 1.248424M\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"xQhujpVa4P95"},"execution_count":null,"outputs":[]}]}